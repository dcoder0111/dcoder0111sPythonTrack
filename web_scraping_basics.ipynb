{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_scraping_basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+47UsJuzMXJaUMob70+IW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freezingMonkeys/freezingMonkeysPythonTrack/blob/main/web_scraping_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY8bVd2HxdIA"
      },
      "source": [
        "# Web Scraping\n",
        "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. In this module, we will scrape basic information using requests, JSON, and XML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfZPHBklM9Yu"
      },
      "source": [
        "---\n",
        "# Extracting data through JSON:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT9hFSANNPY9"
      },
      "source": [
        "First, we import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwV6Lh5DxYt-"
      },
      "source": [
        "import requests\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAkrlpL2NWEV"
      },
      "source": [
        "Then, we set a url of the API we want to scrape. Here, we will use an example from the Hong Kong flights list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-L9XOJsNZCH"
      },
      "source": [
        "date = input(\"input date (yyyy-mm-dd)\")\n",
        "url = f\"https://www.hongkongairport.com/flightinfo-rest/rest/flights/past?date={date}&arrival=true&lang=en&cargo=false\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quX7b6xxNc9h"
      },
      "source": [
        "Use requests to get content of the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuPiJ817NhQh"
      },
      "source": [
        "json_page = requests.get(url).content # type = bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ANjobrNkE8"
      },
      "source": [
        "Use json to load json_page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZSBcWOJNn1p"
      },
      "source": [
        "flight_list = json.loads(json_page)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8T8isz7NtSs"
      },
      "source": [
        "Afterwards, we can treat the list like a normal python list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhWp0_0Xyjti"
      },
      "source": [
        "print(type(flight_list))\n",
        "len(flight_list)\n",
        "\n",
        "# the type is list, so we can extract the first and only element and see its format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pRC9jcYLxC4"
      },
      "source": [
        "type(flight_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zntz7yHZL-F-"
      },
      "source": [
        "# Since it is stored as a dictionary, we can process the data like a normal dictionary\n",
        "\n",
        "# Example:\n",
        "d = flight_list[0]\n",
        "all_flights = d['list']\n",
        "\n",
        "def print_flights(flights):\n",
        "  for i in range(len(flights)):\n",
        "    print(flights[i][\"flight\"][0][\"airline\"])\n",
        "    print(flights[i][\"flight\"][0][\"no\"])\n",
        "    print(flights[i][\"origin\"][0])\n",
        "    print(flights[i][\"status\"])\n",
        "    print()\n",
        "\n",
        "print_flights(all_flights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWowr6CDN-CG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Extract data through XML and BeautifulSoup:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WdSS9zuOJLv"
      },
      "source": [
        "First, we import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwjvmzC-OHJ8"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as soup # this imports BeautifulSoup with the shortened name soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYrcBMm6lKYD"
      },
      "source": [
        "Same steps as before, use requests to get page content. Here, we will use the info from google news. They did not develop a way for us to easily access the information so we will have to scrape from the raw site. This is where BeautifulSoup comes in handy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoVxSro9OWES"
      },
      "source": [
        "url = 'https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en'\n",
        "xml_page = requests.get(url).content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDifw2QKlf6T"
      },
      "source": [
        "Then, we use BeatifulSoup to process the scraped data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwb5JRdWljl5"
      },
      "source": [
        "soup_page = soup(xml_page, \"xml\")\n",
        "\n",
        "soup_page"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGtVChEJl6hl"
      },
      "source": [
        "By printing the page, we find that all of the news are stored in an item tag. Since we want to create a list of news, we just have to extract the item tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RXsawBTlly3"
      },
      "source": [
        "news_list = soup_page.findAll('item')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaCpN_aPmFOX"
      },
      "source": [
        "Afterwards, we can process everything in a format like below (more explanation within code)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SyeolXamEKg"
      },
      "source": [
        "for news in news_list:\n",
        "  # to get specific tags -> item.tag\n",
        "  # .text grabs the text between the <item></item>\n",
        "  print(news.title.text)\n",
        "  print(news.link.text)\n",
        "  # to grab specific item within the tag\n",
        "  print(news.source.get('url'))\n",
        "  print(news.pubDate.text)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}