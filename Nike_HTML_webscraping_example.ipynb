{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nike_HTML_webscraping_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNG8aJeDg3CWavdwNvEjega",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freezingMonkeys/freezingMonkeysPythonTrack/blob/main/Nike_HTML_webscraping_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPxOeznYTC08"
      },
      "source": [
        "<h1>HTML Web Scraping Exercise: Nike üëü</h1>\n",
        "<h2>In the following code, we will scrape a nike page for shoes to practice web scraping through HTML code.</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU3ktNnJTZ6z"
      },
      "source": [
        "Step 0: Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwSyhChspKtU"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES7QkB2BTdbs"
      },
      "source": [
        "Step 1: Import page content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8gO7BUSq2B6"
      },
      "source": [
        "url = 'https://www.nike.com.hk/man/basketball/shoe/list.htm?intpromo=PETP'\n",
        "page = requests.get(url).content\n",
        "soup_page = soup(page,'html')\n",
        "shoes = soup_page.findAll('dl', {'class': 'product_list_content'})\n",
        "for shoe in range(len(shoes)):\n",
        "  print(shoes[shoe].findAll('span', {'class': 'up'})[0].text + \": \"+ shoes[shoe].findAll('dd', {'class': 'color666'})[0].text)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHa5EqTjUsnT"
      },
      "source": [
        "Print page content to analyse:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G-l5snFrnwI"
      },
      "source": [
        "soup_page"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4NWQwUBVnEH"
      },
      "source": [
        "Step 2: Find tags with shoe name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMB41VrBromA"
      },
      "source": [
        "shoes = soup_page.findAll('dl', {'class': 'product_list_content'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_95iqn0esr_9"
      },
      "source": [
        "shoes[0].findAll('span', {'class': 'up'})[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6e3RXGrV0s1"
      },
      "source": [
        "Step 3: Print all shoe names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXPxHnIvs49V"
      },
      "source": [
        "shoes = soup_page.findAll('dl', {'class': 'product_list_content'})\n",
        "for shoe in range(len(shoes)):\n",
        "  print(shoes[shoe].findAll('span', {'class': 'up'})[0].text + \": \"+ shoes[shoe].findAll('dd', {'class': 'color666'})[0].text)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIbmi89yV12y"
      },
      "source": [
        "After finishing this activity, feel free to challenge yourself and follow along the excersise named NikeShoes_homework. It will test your ability to combine your knowledge in Pandas and HTML web scraping and allow for you to analyse the data scraped. Have fun coding! üíª üòÅ"
      ]
    }
  ]
}